---
audio_channels: &audio_channels 2
sample_rate: &sample_rate 48000
segment_duration: 2.
target_stem: "vocals"

train_datasets:
    MUSDB18HQ:
        root: "/datasets/musdb18hq"
        split: "train"
        time_align: "strict"  # "strict" | "group"

test_datasets:
    MUSDB18HQ:
        root: "/datasets/musdb18hq"
        split: "test"
        
sampler: RandomSongSampler

name: MMTransformer1D_2b_flex_window_200

model:
    name: MMTransformer1D_2
    audio_channels: *audio_channels
    sample_rate: *sample_rate
    n_fft: 2048 
    hop_length: 480
    n_bands: 256
    band_dim: 64
    patch_size: [2, 4]
    window_size: [100, 99]
    strides: [1, 2, 4, 2, 1]
    n_layers: [2, 2, 4, 4, 4]
    n_heads: 12
    dim: 384
    use_flex_attention: True

train:
    num_workers: 32
    precision: bf16  # "no" (fp32) | bf16"
    loss: # List of [loss_name, weight]
    - [l1, 1.0]
    - [mrstft, 1.0]
    optimizer: AdamW
    lr: 1e-4
    max_grad_norm: 1.0 # Leave blank if no gradient clipping is used
    warm_up_steps: 1000  # Leave blank if no warm up is used
    batch_size_per_device: 4
    test_every_n_steps: 20000
    save_every_n_steps: 50000
    training_steps: 2000000
    resume_ckpt_path:  # Leave blank if train from scratch

validate:
    audios_num: 5